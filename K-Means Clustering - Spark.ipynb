{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "800cf2d9",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "We'll be working with a real data set about seeds, from UCI repository: https://archive.ics.uci.edu/ml/datasets/seeds.\n",
    "\n",
    "The examined group comprised kernels belonging to three different varieties of wheat: Kama, Rosa and Canadian, 70 elements each, randomly selected for \n",
    "the experiment. High quality visualization of the internal kernel structure was detected using a soft X-ray technique. It is non-destructive and considerably cheaper than other more sophisticated imaging techniques like scanning microscopy or laser technology. The images were recorded on 13x18 cm X-ray KODAK plates. Studies were conducted using combine harvested wheat grain originating from experimental fields, explored at the Institute of Agrophysics of the Polish Academy of Sciences in Lublin. \n",
    "\n",
    "The data set can be used for the tasks of classification and cluster analysis.\n",
    "\n",
    "\n",
    "Attribute Information:\n",
    "\n",
    "To construct the data, seven geometric parameters of wheat kernels were measured: \n",
    "1. area A, \n",
    "2. perimeter P, \n",
    "3. compactness C = 4*pi*A/P^2, \n",
    "4. length of kernel, \n",
    "5. width of kernel, \n",
    "6. asymmetry coefficient \n",
    "7. length of kernel groove. \n",
    "All of these parameters were real-valued continuous.\n",
    "\n",
    "Let's see if we can cluster them in to 3 groups with K-means!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cd46c0",
   "metadata": {},
   "source": [
    "### Importing PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07453005",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-3.1.1-bin-hadoop3.2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9edd370",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59aab2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('clustering').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f6f83f",
   "metadata": {},
   "source": [
    "### Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3656587d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('seeds_dataset.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9178b6",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c055a42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- area: double (nullable = true)\n",
      " |-- perimeter: double (nullable = true)\n",
      " |-- compactness: double (nullable = true)\n",
      " |-- length_of_kernel: double (nullable = true)\n",
      " |-- width_of_kernel: double (nullable = true)\n",
      " |-- asymmetry_coefficient: double (nullable = true)\n",
      " |-- length_of_groove: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c4b327",
   "metadata": {},
   "source": [
    "### Shape of the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f77d0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 7)\n"
     ]
    }
   ],
   "source": [
    "print((df.count(),len(df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b17759a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['area',\n",
       " 'perimeter',\n",
       " 'compactness',\n",
       " 'length_of_kernel',\n",
       " 'width_of_kernel',\n",
       " 'asymmetry_coefficient',\n",
       " 'length_of_groove']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3573a79d",
   "metadata": {},
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "369bb3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d5e6386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+-----------+----------------+---------------+---------------------+----------------+\n",
      "|area|perimeter|compactness|length_of_kernel|width_of_kernel|asymmetry_coefficient|length_of_groove|\n",
      "+----+---------+-----------+----------------+---------------+---------------------+----------------+\n",
      "|   0|        0|          0|               0|              0|                    0|               0|\n",
      "+----+---------+-----------+----------------+---------------+---------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a297913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+-----------+----------------+---------------+---------------------+----------------+\n",
      "| area|perimeter|compactness|length_of_kernel|width_of_kernel|asymmetry_coefficient|length_of_groove|\n",
      "+-----+---------+-----------+----------------+---------------+---------------------+----------------+\n",
      "|15.26|    14.84|      0.871|           5.763|          3.312|                2.221|            5.22|\n",
      "+-----+---------+-----------+----------------+---------------+---------------------+----------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efac6d9d",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5648ab4",
   "metadata": {},
   "source": [
    "### Transforming data for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e814487",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f5e9761",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=df.columns,outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dace1f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = assembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a19e37d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- area: double (nullable = true)\n",
      " |-- perimeter: double (nullable = true)\n",
      " |-- compactness: double (nullable = true)\n",
      " |-- length_of_kernel: double (nullable = true)\n",
      " |-- width_of_kernel: double (nullable = true)\n",
      " |-- asymmetry_coefficient: double (nullable = true)\n",
      " |-- length_of_groove: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfbb7fa",
   "metadata": {},
   "source": [
    "### Scaling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eeefa283",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e16ffb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(inputCol='features',outputCol='scaledFeatures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b1c0ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_model = scaler.fit(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25868d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#New final data \n",
    "final_data = scaler_model.transform(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "58620578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['area',\n",
       " 'perimeter',\n",
       " 'compactness',\n",
       " 'length_of_kernel',\n",
       " 'width_of_kernel',\n",
       " 'asymmetry_coefficient',\n",
       " 'length_of_groove',\n",
       " 'features',\n",
       " 'scaledFeatures']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dbb899",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3476b808",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0c167148",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(featuresCol='scaledFeatures',k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b22c0487",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting the model\n",
    "model = kmeans.fit(final_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b43969",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aea6056f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Center\n",
      "[array([ 4.07497225, 10.14410142, 35.89816849, 11.80812742,  7.54416916,\n",
      "        3.15410901, 10.38031464]), array([ 6.35645488, 12.40730852, 37.41990178, 13.93860446,  9.7892399 ,\n",
      "        2.41585013, 12.29286107]), array([ 4.96198582, 10.97871333, 37.30930808, 12.44647267,  8.62880781,\n",
      "        1.80061978, 10.41913733])]\n"
     ]
    }
   ],
   "source": [
    "print('Cluster Center')\n",
    "print(model.clusterCenters())\n",
    "#We get 3 arrays denoting 3 types of clusters that we defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ada57d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|prediction|\n",
      "+----------+\n",
      "|         2|\n",
      "|         2|\n",
      "|         2|\n",
      "|         2|\n",
      "|         2|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Clusters Created\n",
    "model.transform(final_data).select('prediction').show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
